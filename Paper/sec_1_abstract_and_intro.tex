\section*{Abstract}

Distributed networks can be developed using various methodologies. 
This article aims to compare various client-server models, including: 
Iterative and Managed/Unmanaged concurrent servers, to track and monitor performance based on time taken to efficiently serve a client. 
The iterative model is generally accepted to be efficient for light request loads whereas concurrent models perform better under heavy request loads. 
The objective of our research is to validate these claims by monitoring heavy and light load requests across the various client/server paradigms and provide the data for analysis.

\section{Introduction}

Our team developed three models to monitor and analyze the performance of client request server calls. 
Each of these models were tested independently using the same metrics to accurately monitor performance. 
According to an article by Daniel A. Menasce, 
``To analyze the performance of client/server systems we need to consider three issues: the anatomy of C/S interactions, the software architecture of a server, and the system architecture which includes hardware configuration and networking topologies''
\footnote{
	Menasce. D. A. (2001). Performance of Client/Server Systems. Lecture Notes in Computer Science, Pages 201 - 202.
}.
The goal of our research was to consider the three issues listed above as they pertain to each protocol tested. 

The first model, Iterative, handles both the connection request and the call. 
The iterative model is implemented through the runnable interface and handles requests one at a time. 
This implementation is appropriate for transactions that are simple and do not require long return times; however, performance suffers when transaction times grow. 
This deficiency in performance is attributed to excessive queuing times resulting from multiple requests. 

The second protocol tested was the concurrent model. 
This model functions similarly to the iterative model; a request is made from the client and the server responds. 
The key differentiator being the concurrent server is capable of spawning child processes which allows it to handle more than one task at a time. 
Concurrent server performance under a heavy load is much better than the iterative model as there is no queuing delay. 
However, under a light load, the iterative server outperforms the concurrent server which is attributed to the lack of overhead required to spawn a child thread to handle a task. 

The third protocol was implemented using thread pools. 
Thread pools restrict the amount of overhead down to the initialization of the pool. 
In other words, when the threads are created in the pool, they do not close once a task is complete. 
If a thread has finished performing a task, it becomes available for the next request. 
Thread pools typically result in better performance and stability. 
The results of our research indicate that utilizing the thread pool methodology yields the best results overall when tracking performance compared to overhead.
The goal of any network is to provide a timely response to a request. 
When designing a network, it is imperative the engineer understands exactly what is required to complete the task in a manner that does not compromise speed or stability. 
One may assume that a concurrent server is the best option over and iterative server, but as our research indicates, this is not always the case. 
A delicate balance between design and demand must be met to result in optimal performance.