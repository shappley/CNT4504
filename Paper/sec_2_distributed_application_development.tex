\section{Distributed Application Development}

For this project, we used three different approaches for developing the distributed application: iterative, unmanaged concurrent, and managed concurrent. 
The basic objectives of the project were simple: the server waits for a client to request connection and establishes the connection (Figure 1, line 5), then serves the client in some manner (line 6). 
For this project, serving a client included the client requesting the output from some system command (line 19, 21), the server executing this command (line 23), and returning the results back to the client (line 24).

While each development approach performed the same tasks, the manner in which each did varied slightly. 
Once the server had established a connection with the client, how that client was served was the key differentiator.
The differences, while subtle and small, were found to have large impacts on performance, scalability, and even reliability of the systems involved.

Figure 1:
\begin{lstlisting}
@Override
public void run() {
	while (true) {
		try {
			final Socket client = getNextClient();
			serveClient(client);
		} catch (IOException e) {
			e.printStackTrace();
		}
	}
}

public Socket getNextClient() throws IOException {
	final Socket client = server.accept();
	return client;
}

public void serveClient(Socket client) throws IOException {
	final BufferedReader in = new BufferedReader(new InputStreamReader(client.getInputStream()));
	final PrintStream out = new PrintStream(client.getOutputStream());
	final String read = in.readLine();
	final MenuOption selection = MenuOption.valueOf(read);
	final String response = executeCommand(selection);
	out.println(response);
	out.close();
}
\end{lstlisting}

\subsection{Iterative}

The Iterative server approach is the the simplest of the three. 
It is referred to as ``iterative'' because each client is served iteratively, one at a time, in order. 
The server operates with a single \code{Thread}. 
This Thread is responsible for waiting for a client connection request, establishing the connection once requested, and serving the client. 
Waiting for a client is done using the \code{accept()} method (Figure 1, line 14) found in the \code{SocketServer} class.
This method is known as a \textit{blocking call} because the caller cannot continue until this method finishes. 
That is, \code{accept()} will wait until a client requests connection before continuing.
Once a client connection is established, the same Thread will then serve that client. 
Because the same Thread handles both the connection and the processing, the next client cannot be served until the previous request has completed. Clients will need to \textit{queue}.

\subsection{Concurrent (Unmanaged)}

Concurrent servers are an improvement over Iterative due to a simple, key difference: the client is served on a different Thread than the one that handles the connection requests. 
That is, multiple clients can be served \textit{at the same time}.
The main Thread is still responsible for handling and establishing incoming connection requests, but once a client has connected, a new Thread is created to serve this client (Figure 2, line 3-9). 
This new Thread will run \textit{concurrently}, at the same time, allowing the main Thread to immediately go back to waiting for the next client. 
Clients will not need to queue; as such, there is no \textit{queuing delay}.

Figure 2:
\begin{lstlisting}
@Override
public void serveClient(Socket client) throws IOException {
	new Thread(() -> {
		try {
			super.serveClient(client);
		} catch (IOException e) {
			e.printStackTrace();
		}
	}).start();
}
\end{lstlisting}

While this approach may sound like the best solution, it has its drawbacks: namely the overhead associated with creating a new Thread for each request. 
If, for example, two-hundred clients were to simultaneously make a request, then two-hundred Threads would need to be created. 
This places a lot of strain on the system as the operating system must perform many complex scheduling algorithms for these Threads, which may deprive other applications of the necessary resources to run other, more critical services. 
This is known as \textit{oversubscription} \cite{microsoft}.

\subsection{Thread Pool (Managed Concurrent)}

Managed Concurrent servers can be implemented in several ways, but the objective is the same: limit the number of Threads created to prevent oversubscription. 
In this example, Thread management was achieved using a fixed \textit{Thread Pool}.
A Thread Pool, as the name implies, is a pool of Threads. 
A fixed pool is instantiated with a fixed number of Threads that does not change for the life of the pool.
When you submit a \textit{job} to the pool (Figure 3, line 3-9), a Thread from the pool is removed, used to execute the job, then returned to the pool when the job is complete. 
Each Thread is recycled, removing the need to create a new Thread per-request.
If there are no available Threads in the pool, then the job is queued. 
While this does reintroduce queuing delay as a possibility, it prevents oversubscription, which dramatically improves the stability of the system.

Figure 3:
\begin{lstlisting}
@Override
public void serveClient(Socket client) throws IOException {
	pool.submit(() -> {
		try {
			super.serveClient(client);
		} catch (IOException e) {
			e.printStackTrace();
		}
	});
}
\end{lstlisting} 