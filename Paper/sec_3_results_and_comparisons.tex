\section{Results and Comparison}

\subsection{Test Bed}

Testing was performed on client and server computers with similar specifications. 
Both computers were Dell Optiplex 755 platforms, running the Ubuntu 14.04.4 GNU/Linux operating system. 
The Dells were equipped with Intel Core2 Quad CPU Q9300 processors at 2.50GHz clock speed, with 4 gigabytes of RAM. 
For storage, both machines were equipped with SATA Seagate Barracuda 7200.10 hard drives with a capacity of 160GB, running at 7200 RPM with an 8MB cache at a speed of 3.0Gb/s.

Networking was implemented on the University of North Florida’s VPN, cloudlab.unf.edu, using a Netgear gigabit switch, and both client and server machines were equipped with an Intel 82566DM-2 Gigabit Network Connection. 
The IP address of the client was 192.168.100.111 while the server’s IP was 192.168.100.112.

\subsection{Studies Carried Out}

In order to test the protocols, it is common to use a multistep process 
\cite{stackify}.
Step one is to identify the testing environment. 
All testing was done on the hardware and software provided by the University of North Florida, as specified in section 3.1. 
Second we identify KPIs and performance metrics. 
Performance testing can be broken down into two basic categories, Functional vs. Non Functional testing. 
We were concerned primarily with Non-Functional testing, which tests the readiness of a system as opposed to task-based testing 
\cite{reqtest}.
Non-Functional testing can further be broken down into categories such as load testing, stress testing and spike testing among others. 
For this project we focused on Scalability Testing, which determines how effectively the system handles increasing workloads. 
For corporations with wired, distributed wide area IP data networks, the most requested QoS metrics for business-critical applications are network latency and especially, application response time
\cite{morreale}.
For this test, the metric that was measured was latency, by measuring response time per transaction.

Next it is important to plan how to capture the required metrics. For this test, the team determined to subtract the system time a response was received by the client from the time that the request was sent. This was implemented in the class \code{MultiClientSim} by using a for-loop to iterate through each active client, declaring a \code{startTime} variable using \code{System.currentTimeMillis()} and subtracting an \code{endTime} variable after the server response also using \code{System.currentTimeMillis()}. 

The first series of tests were run using an iterative protocol, measuring the mean server response time as correlating to the number of clients. Both tests stepped through a progression of clients  using the set  $n = \{1, 5, 10, 20, 30, 40, 50, 60, 70, 75\}$.  
The first test measured response times for a \code{netstat} request (graph attachment 1), while the second test measured response time for a \code{date} query (graph attachment 2). 
In the second series of tests, these tests were repeated but with the server process configured to handle multiple client requests concurrently by spawning a thread per request.

\subsection{Results}

The iterative server performs fairly well with a light load requesting date and time, increasing linearly as clients are added and then roughly plateauing after 40 clients. 
The concurrent server does not fare as well with response time rising sharply for the first 30 clients, after which response time levels out. 
Our results showed that the iterative server ranged from a response time of 20 ms to 35 ms as we reached 75 servers.
The concurrent server performed better at higher client levels, peaking at less than 25 ms. 
During the netstat tests, both iterative and concurrent protocol response times increase exponentially. 
For the iterative protocol the response times increase very sharply, reaching over 1000 ms after the first 30 clients to approximately 7500 ms after 75 clients. 
The concurrent protocol clearly performs better on this test, still being around approximately 800 ms after 30 clients and peaking at less than 5000 ms after 75 clients. 

The testing confirmed that the iterative protocol performs better with lighter loads and fewer clients, but the concurrent protocol outperforms as we increase the load and client count.
We saw severe fluctuation during the date and time tests with both protocols between 30 and 75 clients. 
While many performance tests start with a measure of latency, they also typically include other KPIs such as CPU and memory utilization, page file utilization, disk time and amount of free space
\cite{molyneaux}.
Adding some of these extra KPIs on future testing could help explain testing inconsistencies such as fluctuations and high standard deviations.
