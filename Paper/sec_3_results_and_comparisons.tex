\section{Results and Comparison}

\subsection{Test Bed}

Testing was performed on client and server computers with similar specifications. 
Both computers were Dell Optiplex 755 platforms, running the Ubuntu 14.04.4 GNU/Linux operating system. 
The Dells were equipped with Intel Core2 Quad CPU Q9300 processors at 2.50GHz clock speed, with 4 gigabytes of RAM. 
For storage, both machines were equipped with SATA Seagate Barracuda 7200.10 hard drives with a capacity of 160GB, running at 7200 RPM with an 8MB cache at a speed of 3.0Gb/s.

Networking was implemented on the University of North Florida’s VPN, cloudlab.unf.edu, using a Netgear gigabit switch, and both client and server machines were equipped with an Intel 82566DM-2 Gigabit Network Connection. 
The IP address of the client was 192.168.100.111 while the server’s IP was 192.168.100.112.

\subsection{Studies Carried Out}

In order to test the protocols, it is common to use a multistep process 
\cite{stackify}.
Step one is to identify the testing environment. 
All testing was done on the hardware and software provided by the University of North Florida, as specified in section 3.1. 
Second we identify KPIs and performance metrics. 
Performance testing can be broken down into two basic categories, Functional vs. Non Functional testing. 
We were concerned primarily with Non-Functional testing, which tests the readiness of a system as opposed to task-based testing 
\cite{reqtest}.
Non-Functional testing can further be broken down into categories such as load testing, stress testing and spike testing among others. 
For this project we focused on Scalability Testing, which determines how effectively the system handles increasing workloads. 
For corporations with wired, distributed wide area IP data networks, the most requested QoS metrics for business-critical applications are network latency and especially, application response time
\cite{morreale}.
For this test, the metric that was measured was latency, by measuring response time per transaction.

Next it is important to plan how to capture the required metrics. For this test, the team determined to subtract the system time a response was received by the client from the time that the request was sent. This was implemented in the class \code{MultiClientSim} by using a for-loop to iterate through each active client, declaring a \code{startTime} variable using \code{System.currentTimeMillis()} and subtracting an \code{endTime} variable after the server response also using \code{System.currentTimeMillis()}. 

The first series of tests were run using an iterative protocol, measuring the mean server response time as correlating to the number of clients. Both tests stepped through a progression of clients  using the set  $n = \{1, 5, 10, 20, 30, 40, 50, 60, 70, 75\}$.  The first test measured response times for a \code{netstat} request (graph attachment 1), while the second test measured response time for a \code{date} query (graph attachment 2). In the second series of tests, these tests were repeated but with the server process configured to handle multiple client requests concurrently by spawning a thread per request.

For the final round of tests, the same tests from the prior two tests were repeated using a managed concurrent server, implemented using Thread Pools. The size of the pool was fixed at 16 threads.
TODO: put some more stuff here about thread pool tests.